{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2439f86-0d31-4336-81e3-488eb2c8016f",
   "metadata": {},
   "source": [
    "# Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226c0d12-550e-4864-997a-29906fa07c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # For numerical operations\n",
    "\n",
    "# Import all specific function and libraries from custom modules\n",
    "from testCases import *  # Import all test cases for the unit testing\n",
    "from gc_utils import sigmoid, relu, vector_to_dictionary, dictionary_to_vector, gradients_to_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9eeae-da94-4891-875a-3d7127ab9a8d",
   "metadata": {},
   "source": [
    "## 1) How does gradient checking works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b576ec2-1751-4c72-a330-dee4bbbd7ce7",
   "metadata": {},
   "source": [
    "## 2) 1-dimensional gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f03082-dc69-4147-8ed0-b1b0efe473e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary library\n",
    "import numpy as np  # NumPy is used for numerical operations\n",
    "\n",
    "def forward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Implements linear forward propagation to compute the cost function J.\n",
    "    \n",
    "    J(theta) = theta * x\n",
    "    \n",
    "    Arguments:\n",
    "    x -- a scalar or vector, representing the input(s)\n",
    "    theta -- a scalar or vector, representing the parameter(s)\n",
    "    \n",
    "    Returns:\n",
    "    J -- the computed value of the function J\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the linear combination of inputs and parameters\n",
    "    J = np.dot(theta, x)\n",
    "    \n",
    "    return J\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69d9545e-acac-4edf-9ec0-e60bf1ae65d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J = 8\n"
     ]
    }
   ],
   "source": [
    "# Define the input values\n",
    "x, theta = 2, 4  # x is the input, theta is the parameter\n",
    "\n",
    "# Compute the value of J using the forward_propagation function\n",
    "J = forward_propagation(x, theta)\n",
    "\n",
    "# Output the result\n",
    "print(f\"J = {J}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "920a883b-0ac5-45f0-b25e-8f5f6e607ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the cost function J with respect to the parameter theta.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- a scalar input\n",
    "    theta -- a scalar parameter\n",
    "    \n",
    "    Returns:\n",
    "    dtheta -- the computed gradient of the cost with respect to theta\n",
    "    \"\"\"\n",
    "    \n",
    "    # The derivative of J(theta) = theta * x with respect to theta is simply x\n",
    "    dtheta = x\n",
    "    \n",
    "    return dtheta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfd3a8f4-e993-4596-a7e8-cda77ab1ee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtheta = 2\n"
     ]
    }
   ],
   "source": [
    "# Define input values\n",
    "x, theta = 2, 4  # x is the input value, theta is the parameter\n",
    "\n",
    "# Compute the gradient of the cost function with respect to theta\n",
    "dtheta = backward_propagation(x, theta)\n",
    "\n",
    "# Print the result\n",
    "print(f\"dtheta = {dtheta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce3bec8e-7c8d-448d-bde2-49f63fc7eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Import NumPy for numerical operations\n",
    "\n",
    "def gradient_check(x, theta, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Performs gradient checking by comparing the analytical gradient (from backward propagation)\n",
    "    to the numerical gradient (calculated using the finite difference approximation).\n",
    "    \n",
    "    Arguments:\n",
    "    x -- a scalar input\n",
    "    theta -- a scalar parameter\n",
    "    epsilon -- a small value used for computing the numerical gradient approximation (default is 1e-7)\n",
    "    \n",
    "    Returns:\n",
    "    difference -- the relative difference between the numerical gradient and the analytical gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the numerical gradient using the centered difference formula\n",
    "    thetaplus = theta + epsilon    # Increment theta by epsilon\n",
    "    thetaminus = theta - epsilon   # Decrement theta by epsilon\n",
    "    J_plus = np.dot(thetaplus, x)  # Compute J with theta incremented\n",
    "    J_minus = np.dot(thetaminus, x)  # Compute J with theta decremented\n",
    "    gradapprox = (J_plus - J_minus) / (2 * epsilon)  # Numerical gradient approximation\n",
    "    \n",
    "    # Compute the analytical gradient using backward propagation\n",
    "    grad = x  # Analytical gradient of J with respect to theta\n",
    "    \n",
    "    # Compute the difference between the numerical and analytical gradients\n",
    "    numerator = np.linalg.norm(gradapprox - grad)  # Step 1': Compute the norm of the difference\n",
    "    denominator = np.linalg.norm(gradapprox) + np.linalg.norm(grad)  # Step 2': Compute the norm of the sum\n",
    "    difference = numerator / denominator  # Step 3': Relative difference\n",
    "    \n",
    "    # Print the result of the gradient check\n",
    "    if difference < 1e-7:\n",
    "        print(\"The gradient is correct!\")\n",
    "    else:\n",
    "        print(\"The gradient is wrong!\")\n",
    "    \n",
    "    return difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b6ae6a7-cf10-4d51-bb76-e7dc197ba017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient is correct!\n",
      "difference = 2.919335883291695e-10\n"
     ]
    }
   ],
   "source": [
    "# Define input values\n",
    "x, theta = 2, 4  # x is the input value, theta is the parameter\n",
    "\n",
    "# Perform gradient checking\n",
    "difference = gradient_check(x, theta)\n",
    "\n",
    "# Print the result of the gradient check\n",
    "print(f\"difference = {difference}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e1afc-387b-4c89-a48b-438e025c6235",
   "metadata": {},
   "source": [
    "## 3) N-dimensional gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b3f8e24-0c57-4261-9598-1a10a87d12bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forward_propagation_n(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    Implements forward propagation and computes the logistic cost function.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Input data of shape (number of features, number of examples)\n",
    "    Y -- Labels of shape (1, number of examples)\n",
    "    parameters -- Dictionary containing:\n",
    "        W1 -- Weight matrix of shape (5, 4)\n",
    "        b1 -- Bias vector of shape (5, 1)\n",
    "        W2 -- Weight matrix of shape (3, 5)\n",
    "        b2 -- Bias vector of shape (3, 1)\n",
    "        W3 -- Weight matrix of shape (1, 3)\n",
    "        b3 -- Bias vector of shape (1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- The logistic cost function for the given examples\n",
    "    cache -- A tuple containing intermediate values for backward propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters from the dictionary\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)  # Activation after first layer\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)  # Activation after second layer\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)  # Final activation\n",
    "\n",
    "    # Compute the cost function\n",
    "    logprobs = -np.log(A3) * Y - np.log(1 - A3) * (1 - Y)\n",
    "    cost = np.sum(logprobs) / X.shape[1]  # Average cost per example\n",
    "\n",
    "    # Store intermediate values for backward propagation\n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return cost, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e040acf-c6db-4674-8f57-bb0af0ed8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Implements backward propagation to calculate the gradients of the cost function with respect to the model parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Input data of shape (input size, number of examples)\n",
    "    Y -- True labels of shape (1, number of examples)\n",
    "    cache -- Tuple containing intermediate values from forward_propagation_n()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- Dictionary containing the gradients of the cost with respect to each parameter, activation, and pre-activation variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve cached values from forward propagation\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    m = X.shape[1]  # Number of examples\n",
    "    \n",
    "    # Backward propagation: compute gradients for each layer\n",
    "    \n",
    "    # Layer 3 (Output layer): dZ3, dW3, db3\n",
    "    dZ3 = A3 - Y  # Gradient of cost with respect to Z3\n",
    "    dW3 = np.dot(dZ3, A2.T) / m  # Gradient of cost with respect to W3\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True) / m  # Gradient of cost with respect to b3\n",
    "    \n",
    "    # Layer 2: dA2, dZ2, dW2, db2\n",
    "    dA2 = np.dot(W3.T, dZ3)  # Gradient of cost with respect to A2\n",
    "    dZ2 = dA2 * (A2 > 0)  # Gradient of cost with respect to Z2 (ReLU derivative)\n",
    "    dW2 = np.dot(dZ2, A1.T) / m  # Gradient of cost with respect to W2\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m  # Gradient of cost with respect to b2\n",
    "    \n",
    "    # Layer 1: dA1, dZ1, dW1, db1\n",
    "    dA1 = np.dot(W2.T, dZ2)  # Gradient of cost with respect to A1\n",
    "    dZ1 = dA1 * (A1 > 0)  # Gradient of cost with respect to Z1 (ReLU derivative)\n",
    "    dW1 = np.dot(dZ1, X.T) / m  # Gradient of cost with respect to W1\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m  # Gradient of cost with respect to b1\n",
    "    \n",
    "    # Store gradients in a dictionary\n",
    "    gradients = {\n",
    "        \"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "        \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "        \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1\n",
    "    }\n",
    "    \n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e21746a-867d-4eac-9a2f-c2de06d483a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Verifies the correctness of the backward_propagation_n function by comparing its gradients \n",
    "    with numerically approximated gradients.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- dictionary containing model parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "    gradients -- dictionary containing gradients calculated by backward_propagation_n\n",
    "    X -- input data of shape (input size, number of examples)\n",
    "    Y -- true label vector of shape (1, number of examples)\n",
    "    epsilon -- small shift used to compute numerical gradients\n",
    "    \n",
    "    Returns:\n",
    "    difference -- relative difference between the analytical gradients and numerical gradients\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flatten parameters and gradients to vectors\n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    grad_vector = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "\n",
    "    # Initialize arrays for numerical gradient calculation\n",
    "    gradapprox = np.zeros_like(parameters_values)\n",
    "\n",
    "    # Compute the numerical gradients\n",
    "    for i in range(num_parameters):\n",
    "        # Compute J_plus[i]\n",
    "        thetaplus = np.copy(parameters_values)\n",
    "        thetaplus[i] += epsilon\n",
    "        J_plus, _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))\n",
    "        \n",
    "        # Compute J_minus[i]\n",
    "        thetaminus = np.copy(parameters_values)\n",
    "        thetaminus[i] -= epsilon\n",
    "        J_minus, _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))\n",
    "        \n",
    "        # Compute the gradient approximation\n",
    "        gradapprox[i] = (J_plus - J_minus) / (2 * epsilon)\n",
    "    \n",
    "    # Compute the difference between analytical and numerical gradients\n",
    "    numerator = np.linalg.norm(grad_vector - gradapprox)\n",
    "    denominator = np.linalg.norm(grad_vector) + np.linalg.norm(gradapprox)\n",
    "    difference = numerator / denominator\n",
    "\n",
    "    # Print the result with an appropriate message\n",
    "    if difference > 2e-7:\n",
    "        print(f\"\\033[93mThere is a mistake in the backward propagation! difference = {difference}\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[92mYour backward propagation works perfectly fine! difference = {difference}\\033[0m\")\n",
    "    \n",
    "    return difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "338c5ffe-53c2-47a0-a4e5-f8ce5f8b18cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour backward propagation works perfectly fine! difference = 1.1909939370580298e-07\u001b[0m\n",
      "Difference between gradients: 1.1909939370580298e-07\n"
     ]
    }
   ],
   "source": [
    "# Example test case\n",
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "\n",
    "# Perform forward propagation to compute the cost and cache the necessary values\n",
    "cost, cache = forward_propagation_n(X, Y, parameters)\n",
    "\n",
    "# Perform backward propagation to compute gradients with respect to the parameters\n",
    "gradients = backward_propagation_n(X, Y, cache)\n",
    "\n",
    "# Check the gradients using numerical approximation to ensure correctness\n",
    "difference = gradient_check_n(parameters, gradients, X, Y)\n",
    "\n",
    "# Print the difference between the analytical and numerical gradients\n",
    "print(f\"Difference between gradients: {difference}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320273fe-5cb7-4ab6-bcea-8e1182962d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
